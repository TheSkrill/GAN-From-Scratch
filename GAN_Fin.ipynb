{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Fin.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uLhdzdk4V0e",
        "colab_type": "code",
        "outputId": "41c30ae0-422a-4872-d8d1-3e173b74275c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x, derivative = False):\n",
        "    if derivative == True: return x(1-x)\n",
        "    return (1/(1 + np.exp(-x)))\n",
        "\n",
        "\n",
        "def think(input, layer_1, layer_2, bias_1, bias_2):\n",
        "    l1 = sigmoid(np.dot(input, layer_1) + bias_1)\n",
        "    l2 = sigmoid(np.dot(l1, layer_2) + bias_2)\n",
        "    return l2\n",
        "\n",
        "def to_binary(mat):\n",
        "    mat[mat > 0.5] = 1\n",
        "    mat[mat < 0.5] = 0\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "#data \n",
        "X = np.array([[0,0,0], [0,0,1], [0,1,0], [0,1,1], [1,0,0], [1,0,1], [1,1,0], [1,1,1]])\n",
        "y = np.array([[0, 1, 1, 0, 1, 0, 0, 1]])\n",
        "\n",
        "\n",
        "#Defining discriminator\n",
        "\n",
        "#dimensions\n",
        "#   X(input) = 8*3 (8 examples)\n",
        "#   Layer_1  = 3*6\n",
        "#   Layer_2  = 6*1\n",
        "#   Output = 0/1\n",
        "\n",
        "#initalizing weights and bias for discriminator\n",
        "weights_layer_1 = np.random.random((3 , 6))\n",
        "weights_layer_2 = np.random.random((6 , 1))\n",
        "\n",
        "bias_layer_1 = np.random.random((8, 6))\n",
        "bias_layer_2 = np.random.random((8, 1))\n",
        "\n",
        "def disc_train(input, output, layer_1, layer_2, bias_1, bias_2):\n",
        "    \n",
        "    m = len(input) #number of examples\n",
        "    alpha = 0.001  #learning rate\n",
        "\n",
        "    for i in range(10000):\n",
        "        #forward propogation\n",
        "        l1 = sigmoid(np.dot(input, layer_1) + bias_1)            #(input)8*3 X (layer_1)3*6 = (l1)8*6 + (b1)8*6\n",
        "        l2 = sigmoid(np.dot(l1, layer_2) + bias_2)               #(l1)8*6 X (layer2)6*1 = (l2)8*1 + (b2)8*1\n",
        "\n",
        "        loss = -np.sum(np.dot(output, np.log(l2)) + np.dot(1 - output, np.log(1 - l2))) / m\n",
        "\n",
        "        #back propogation\n",
        "        delta_z2 = np.subtract(l2,output.T)                           #(delta_z2)8*1\n",
        "        #print(np.shape(delta_z2))\n",
        "        delta_w2 = np.dot(l1.T, delta_z2) / m                       #(delta_w2)6*1\n",
        "        delta_b2 = np.sum(delta_z2, axis= 1, keepdims=True) / m     #(delta_b2)8*1            \n",
        "\n",
        "        delta_z1 = np.dot(delta_z2, layer_2.T) * l1                 #(delta_z1)8*6     \n",
        "        delta_w1 = np.dot(input.T, delta_z2) / m                    #(delta_w1)3*6\n",
        "        delta_b1 = np.sum(delta_z1, axis= 1, keepdims=True) / m     #(delta_b1)8*6\n",
        "\n",
        "        layer_2 -= alpha * delta_w2\n",
        "        layer_1 -= alpha * delta_w1\n",
        "\n",
        "        bias_1 -= alpha * delta_b1\n",
        "        bias_2 -= alpha * delta_b2 \n",
        "\n",
        "        #if i%100 == 0 :  print(\"At iteration: {0},  Loss: {1}\".format(i, loss))\n",
        "\n",
        "    return (layer_1, layer_2, bias_1, bias_2)\n",
        "\n",
        "print(\"Before training\")\n",
        "print(to_binary(think(X, weights_layer_1, weights_layer_2, bias_layer_1, bias_layer_2)))\n",
        "\n",
        "print(\"Training Discriminator\")\n",
        "disc_weights_layer_1, disc_weights_layer_2, disc_bias_layer_1, disc_bias_layer_2 = disc_train(X, y, weights_layer_1, weights_layer_2, bias_layer_1, bias_layer_2)\n",
        "\n",
        "print(\"After training\")\n",
        "print(to_binary(think(X, disc_weights_layer_1, disc_weights_layer_2, disc_bias_layer_1, disc_bias_layer_2)))\n",
        "\n",
        "\n",
        "\n",
        "#Defining generator\n",
        "\n",
        "#dimensions\n",
        "#   Z(random input) = 8*3 (8 examples)\n",
        "#   Layer_1         = 3*6\n",
        "#   Layer_2         = 6*3\n",
        "#   Output          = 3*1\n",
        "\n",
        "#initializing Z (random noise)\n",
        "Z = np.random.random((8, 3))  #like [0.3, 0.2, 0.1] * 8 examples\n",
        "Y = np.array([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
        "#initalizing weights and bias for generator\n",
        "weights_layer_1 = np.random.random((3 , 6))\n",
        "weights_layer_2 = np.random.random((6 , 3))\n",
        "\n",
        "bias_layer_1 = np.random.random((8, 6))\n",
        "bias_layer_2 = np.random.random((8, 3))\n",
        "\n",
        "print(\"Before training generative network\")\n",
        "print(to_binary(think(Z, weights_layer_1, weights_layer_2, bias_layer_1, bias_layer_2)))\n",
        "\n",
        "\n",
        "def gen_train(input, output, layer_1, layer_2, bias_1, bias_2):\n",
        "    alpha = 0.09\n",
        "    m = len(input)\n",
        "    for i in range(10000):\n",
        "        #forward propogation\n",
        "        l1_Gen      = np.dot(input, layer_1) + bias_1                                      #(input)8*3 X (layer_1)3*6 = (l1)8*6 + (b1)8*6\n",
        "        l1_Gen_sigm = sigmoid(l1_Gen)\n",
        "        l2_Gen      = np.dot(l1_Gen_sigm, layer_2) + bias_2                                      #(l1)8*6 X (layer2)6*1 = (l2)8*1 + (b2)8*1\n",
        "        l2_Gen_sigm = sigmoid(l2_Gen)\n",
        "\n",
        "        #sending the generated [1 , 0, 0] output to trained layers of Discriminator\n",
        "        l1_Disc      = np.dot(l2_Gen_sigm, disc_weights_layer_1) + disc_bias_layer_1                 #(l2_Gen)8*3 X (disc_layer_1)3*6 = (l1_Disc)8*6 + (b1)8*6\n",
        "        l1_Disc_sigm = sigmoid(l1_Disc) \n",
        "        l2_Disc      = np.dot(l1_Disc_sigm, disc_weights_layer_2) + disc_bias_layer_2           #(l1_Disc)8*6 X (disc_layer_2)6*1 = (l2_Disc)8*1 + (b2)8*1\n",
        "        l2_Disc_sigm = sigmoid(l2_Disc)\n",
        "\n",
        "        loss = -np.sum(np.log(1 - l2_Disc_sigm)) / m\n",
        "\n",
        "        #Backpropogation\n",
        "        delta_z2_a = ((-1/ l2_Disc_sigm) * np.dot(sigmoid(l2_Disc), disc_weights_layer_2.T) * l1_Disc_sigm).dot(disc_weights_layer_1.T)\n",
        "        delta_z2_b = sigmoid(l2_Gen)\n",
        "        delta_z2_c = l1_Gen_sigm\n",
        "        delta_w2   = np.dot(delta_z2_c.T, delta_z2_a * delta_z2_b)\n",
        "        delta_b2   = delta_z2_a * delta_z2_b\n",
        "\n",
        "        delta_z1_a = np.dot(delta_z2_a * delta_z2_b, layer_2.T)\n",
        "        delta_z1_b = sigmoid(l1_Gen)\n",
        "        delta_z1_c = input\n",
        "        delta_w1   = np.dot(delta_z1_c.T, delta_z1_a * delta_z1_b)\n",
        "        delta_b1   = delta_z1_a * delta_z1_b\n",
        "\n",
        "        layer_1 -= alpha * delta_w1\n",
        "        layer_2 -= alpha * delta_w2\n",
        "        bias_1  -= alpha * delta_b1\n",
        "        bias_2  -= alpha * delta_b2\n",
        "\n",
        "        if i%1000 == 0 :  print(\"At iteration: {0},  Loss: {1}\".format(i, loss))\n",
        "\n",
        "    return(layer_1, layer_2, bias_1, bias_2)\n",
        "\n",
        "\n",
        "gen_weights_layer_1, gen_weights_layer_2, gen_bias_layer_1, gen_bias_layer_2 = gen_train(Z, Y, weights_layer_1, weights_layer_2, bias_layer_1, bias_layer_2)\n",
        "\n",
        "print(\"After training generative network\")\n",
        "print(to_binary(think(Z, gen_weights_layer_1, gen_weights_layer_2, gen_bias_layer_1, gen_bias_layer_2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before training\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "Training Discriminator\n",
            "After training\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Before training generative network\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "At iteration: 0,  Loss: 0.8282914027938628\n",
            "At iteration: 1000,  Loss: 0.8340133668250375\n",
            "At iteration: 2000,  Loss: 0.834019840269943\n",
            "At iteration: 3000,  Loss: 0.834021966747668\n",
            "At iteration: 4000,  Loss: 0.8340230246226941\n",
            "At iteration: 5000,  Loss: 0.8340236578147808\n",
            "At iteration: 6000,  Loss: 0.834024079385534\n",
            "At iteration: 7000,  Loss: 0.83402438027605\n",
            "At iteration: 8000,  Loss: 0.8340246058407135\n",
            "At iteration: 9000,  Loss: 0.8340247812328558\n",
            "After training generative network\n",
            "[[1. 0. 1.]\n",
            " [1. 0. 1.]\n",
            " [1. 0. 1.]\n",
            " [1. 0. 1.]\n",
            " [1. 0. 1.]\n",
            " [1. 0. 1.]\n",
            " [1. 0. 1.]\n",
            " [1. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G4nXtZq47MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}